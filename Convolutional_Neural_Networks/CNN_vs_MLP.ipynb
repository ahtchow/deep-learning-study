{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layered Perceptron vs Convolutional Neural Networks\n",
    "\n",
    "- CNNs perform way better in general\n",
    "- MLP struggle to handle: Messy Images (e.g Uncentered Images)\n",
    "- In MLP we simply flattened the images down into a single dimension array\n",
    "- CNNs take consideration of changing patterns of pixels.\n",
    "- Pixels closer together are more heavily related.\n",
    "\n",
    "***OVR: CNN can handle more complex datasets compared to MLPs***\n",
    "\n",
    "MLP Issues:\n",
    "    1. Only Uses Fully Connected Layers\n",
    "    2. Doesnt use the two-dimensional aspect to its advantage\n",
    "  \n",
    " We need a new way to process images.\n",
    " \n",
    "CNN:\n",
    "    1. Uses sparesely connected layer\n",
    "    2. Accept matrices as input\n",
    "    \n",
    "    \n",
    "## Local Connectivity\n",
    "\n",
    "CNN:\n",
    "    1. Inputs are instead a matrix which represents an important part of the image\n",
    "    2. Hidden Nodes contain elements that are only connected to a  single hidden layer node (less weights)\n",
    "\n",
    "Before each perceptron was incharge of understand the entire image. With this new local connection, each hidden node is responsible for classifying one part of the image. The several hidden nodes will then report to the single output node which puts the results together.\n",
    "\n",
    "Ovr: Less prone to overfitting, less parameters\n",
    "\n",
    "![q](cnn_img/q10.png)\n",
    "\n",
    "### Weight Sharing\n",
    "\n",
    "It is useful to allow weight sharing between classifications of hidden node (colours) to solve the problem of image being non-centered. It will eliminate the implication that a cat placed in the top-left corner of an img is different from a bott-right cat.\n",
    "\n",
    "\n",
    "## Filters and the Convolutional Layer\n",
    "\n",
    "MLPs only look at individual input values, CNN can look at images as a whole or patches.\n",
    "\n",
    "***Convolutional Layer*** - Key to preserving the images once broken into patches. Applies kernels to scan images uniformly to find features.\n",
    "\n",
    "Produces filtered images with extracted features. E.g Learning a number:\n",
    "\n",
    "![q](cnn_img/q11.png)\n",
    "\n",
    "## Filters & Edges\n",
    "\n",
    "To detect changes in intensity in an image, you’ll be using and creating specific ***image filters*** that look at groups of pixels and react to alternating patterns of dark/light pixels. These filters produce an output that shows edges of objects and differing textures.\n",
    "\n",
    "Spacial Patterns:\n",
    "    1. Color\n",
    "    2. Shape - Patterns of Intensity in an Image (Shape Boundaries)\n",
    "    \n",
    "## Frequency in Images\n",
    "\n",
    "We have an intuition of what frequency means when it comes to sound. High-frequency is a high pitched noise, like a bird chirp or violin. And low frequency sounds are low pitch, like a deep voice or a bass drum. For sound, frequency actually refers to how fast a sound wave is oscillating; oscillations are usually measured in cycles/s (Hz), and high pitches and made by high-frequency waves. Examples of low and high-frequency sound waves are pictured below. On the y-axis is amplitude, which is a measure of sound pressure that corresponds to the perceived loudness of a sound, and on the x-axis is time.\n",
    "\n",
    "![q](cnn_img/q12.png)\n",
    "\n",
    "Similarly, frequency in images is a ***rate of change***. But, what does it means for an image to change? Well, images change in space, and a high frequency image is one where the ***intensity changes a lot***. And the level of brightness changes quickly from one pixel to the next. A low frequency image may be one that is relatively uniform in brightness or changes very slowly. This is easiest to see in an example.\n",
    "\n",
    "***Low Frequency*** - Uniform Pixel Intensity (Brightness)\n",
    "\n",
    "***High Frequency*** - Varying Pixel Intensity (Brightness)\n",
    "\n",
    "![q](cnn_img/q13.png)\n",
    "\n",
    "Most images have both high-frequency and low-frequency components. In the image above, on the scarf and striped shirt, we have a high-frequency image pattern; this part changes very rapidly from one brightness to another. Higher up in this same image, we see parts of the sky and background that change very gradually, which is considered a smooth, low-frequency pattern.\n",
    "\n",
    "High-frequency components also correspond to the edges of objects in images, which can help us classify those objects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# High Pass Filters\n",
    "\n",
    "Filters are used to\n",
    "\n",
    "    1. Fiilter Unwanted Information\n",
    "    2. Amplify features such as boundaries\n",
    " \n",
    "High pass gilters are used to sharpen images and enhance high frequency of images. We will be using these filters on gray scale images. Here is what a HPF applied to an image looks like:\n",
    "\n",
    "![q](cnn_img/q14.png)\n",
    "\n",
    "***Edges**: Areas where intensities change very quickly, object boundaries\n",
    "\n",
    "***Convolution Kernals***: A kernal matrix that modifies an image\n",
    "\n",
    " - A convolution kernal will sum up a given areas pixel intensities and associate a positive or negative darkening affect based on the result. \n",
    " \n",
    "***Kernal Covolution***: Passing kernals over images and changing their values to result in blurring or edge detection\n",
    "\n",
    "![q](cnn_img/q15.png)\n",
    "\n",
    "A specific filter (matrix) with weights slide over a region of the image can is multiplied with the image's pixel intensities. The values are then summed up and blurred/drawn out based on the change of intensities pixel by pixel. In the kernal the center pixel has the most weight. Nearby pixels are negative to measure the difference in intensities. \n",
    "\n",
    "The sum of the pixel is then used as the new intensity for the center pixel. The kernal must slide over every pixel in the image with this method.\n",
    "\n",
    "\n",
    "### Edge Handling\n",
    "Kernel convolution relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge? Well, there are a number of ways to process the edges, which are listed below. It’s most common to use padding, cropping, or extension. In extension, the border pixels of an image are copied and extended far enough to result in a filtered image of the same size as the original image.\n",
    "\n",
    "\n",
    "Solutions: \n",
    "\n",
    "1. ***Extend*** The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.\n",
    "\n",
    "2. ***Padding*** The image is padded with a border of 0's, black pixels.\n",
    "\n",
    "3. ***Crop*** Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped.\n",
    "\n",
    "\n",
    "Now that you know the basics of high-pass filters, let's see if you can choose the best one for a given task.\n",
    "Of the four kernels pictured above, which would be best for finding and enhancing horizontal edges and lines in an image?\n",
    "\n",
    "![q](cnn_img/q17.png)\n",
    "\n",
    "Ans: D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
