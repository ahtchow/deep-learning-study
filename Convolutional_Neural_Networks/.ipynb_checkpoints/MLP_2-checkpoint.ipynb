{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "How many epochs should you train with?\n",
    "\n",
    "Use a training, validation, and test set.\n",
    "\n",
    "1. Training - Used to train model\n",
    "2. Validation - Tells us how well the model is generalizing while the model is training\n",
    "3. Test - Check accuracy after the model is trained\n",
    "\n",
    "It is important to look at the losses relative to each other to know when to stop. In this case, stop around 100.\n",
    "\n",
    "![q](cnn_img/q7.png)\n",
    "\n",
    "## Validation Set: Takeaways\n",
    "We create a validation set to\n",
    "\n",
    "Measure how well a model generalizes, during training\n",
    "Tell us when to stop training a model; when the validation loss stops decreasing (and especially when the validation loss starts increasing and the training loss is still decreasing)\n",
    "\n",
    "There is a way to tell the model to stop training by using the validation data set losses as our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler # Helps Randomize Valdiation Set\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_data)\n",
    "indices = list(range(num_train)) # Create a list of indices\n",
    "np.random.shuffle(indices) # Shuffle\n",
    "split = int(np.floor(valid_size * num_train)) # Calculate Split size\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Define Samplers\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 512\n",
    "        hidden_2 = 512\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 10)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.952880 \tValidation Loss: 0.381055\n",
      "Validation loss decreased (inf --> 0.381055).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.358773 \tValidation Loss: 0.280473\n",
      "Validation loss decreased (0.381055 --> 0.280473).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.282861 \tValidation Loss: 0.232941\n",
      "Validation loss decreased (0.280473 --> 0.232941).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.234109 \tValidation Loss: 0.195092\n",
      "Validation loss decreased (0.232941 --> 0.195092).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.199125 \tValidation Loss: 0.170588\n",
      "Validation loss decreased (0.195092 --> 0.170588).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.175659 \tValidation Loss: 0.160440\n",
      "Validation loss decreased (0.170588 --> 0.160440).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.152874 \tValidation Loss: 0.134890\n",
      "Validation loss decreased (0.160440 --> 0.134890).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.136255 \tValidation Loss: 0.121645\n",
      "Validation loss decreased (0.134890 --> 0.121645).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.124773 \tValidation Loss: 0.113411\n",
      "Validation loss decreased (0.121645 --> 0.113411).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.112534 \tValidation Loss: 0.104182\n",
      "Validation loss decreased (0.113411 --> 0.104182).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new training algorithm will save a model as long as ***valid_loss <= valid_loss_min*** , such that the validation loss is decreasing. This will result in the lowest validation loss model saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load the saved model like\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "# This will be use full for large epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
