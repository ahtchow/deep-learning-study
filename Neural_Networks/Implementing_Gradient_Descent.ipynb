{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Loss vs Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other error functions used for neural networks. Let me teach you another one, called the mean squared error. As the name says, this one is the mean of the squares of the differences between the predictions and the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Squared Errors\n",
    "\n",
    "Goal: Make predictions as close as possible to the real values.\n",
    "\n",
    "To measure, we use a metric of how wrong the predictions are using \"error.\" A common metric is the sum of the squared errors.\n",
    "\n",
    "![a](Images/a1.png)\n",
    "\n",
    "- where y_hat is the prediction and y is thre true value.\n",
    "- you take the sum over all output units j and another sum over all data points μ\n",
    "\n",
    "\n",
    "- This variable j represents the output units of the network. For each output unit, find the difference between the true value yy and the predicted value from the network y_hat, then square the difference, then sum up all those squares.\n",
    "\n",
    "\n",
    "- μ is a sum over all the data points. So, for each data point you calculate the inner sum of the squared differences for each output unit. Then you sum up those squared differences for each data point. That gives you the overall error for all the output predictions for all the data points.\n",
    "\n",
    "## Why Squared Errors (SSE)\n",
    "\n",
    "1. The square ensures the error is always positive, so we only consider magnitude\n",
    "\n",
    "\n",
    "Remember that the output of a neural network, the prediction, depends on weights.\n",
    "\n",
    "![a](Images/a2.png)\n",
    "\n",
    "Our goals is find the weights W_ij that minimize the squared error E. To do this with a neural network we need ***gradient descent***\n",
    "\n",
    "We must take small steps everytime to minimize the error. We can find this direction by calculating the gradient of the squared error. Gradient is another term for rate of change or slope.\n",
    "\n",
    "## Gradient \n",
    "\n",
    "The gradient is just a derivative generalized to functions with more than one variable. We can use calculus to find the gradient at any point in our error function, which depends on the input weights.\n",
    "\n",
    "Below I've plotted an example of the error of a neural network with two inputs, and accordingly, two weights. You can read this like a topographical map where points on a contour line have the same error and darker contour lines correspond to larger errors.\n",
    "\n",
    "At each step, you calculate the error and the gradient, then use those to determine how much to change each weight. Repeating this process will eventually find weights that are close to the minimum of the error function, the black dot in the middle.\n",
    "\n",
    "![a](Images/a3.png)\n",
    "\n",
    "### Caveats / Local Mins\n",
    "\n",
    "Since the weights will just go wherever the gradient takes them, they can end up where the error is low, but not the lowest. These spots are called local minima. If the weights are initialized with the wrong values, gradient descent could lead the weights into a local minimum, illustrated below. There are methods to avoid this, such as using [momentum](https://distill.pub/2017/momentum/).\n",
    "\n",
    "![a](Images/a4.png)\n",
    "\n",
    "\n",
    "## The Math Explained\n",
    "\n",
    "First we need some measure of how bad our prediction are. Just use the difference in output $E = (y - \\hat{y})$. We can also add a square to remove negatives and penalize outliers more, so $E = (y - \\hat{y})^2$.\n",
    "\n",
    "\n",
    "Next we need to sum up the error for all data records denoted by the sum over μ. Also multiply 1/2 to clean up math later.\n",
    "\n",
    "$E = (1/2) \\sum_{μ}(y - \\hat{y})^2$.\n",
    "\n",
    "Next sub in the prediction as a function of weights \n",
    "\n",
    "![a](Images/a5.png)\n",
    "\n",
    "So for a single output: \n",
    "\n",
    "![a](Images/a6.png)\n",
    "\n",
    "Next we find the negative for the gradient, which points to the weights with lowest error.\n",
    "\n",
    "![a](Images/a7.png)\n",
    "\n",
    "### Update Step\n",
    "\n",
    "The update step for each weight parameter can be given as following\n",
    "\n",
    "![a](Images/a8.png)\n",
    "\n",
    "WE must use chain rule to expand the partial derivative. For\n",
    "\n",
    "![a](Images/a9.png)\n",
    "\n",
    "The resulting expansion for the partial derivative of the error with respect weights\n",
    "\n",
    "![a](Images/a10.png)\n",
    "\n",
    "$\\hat{y}$ depends on the weights so we take the partial derivative of y with respect to the weights.\n",
    "\n",
    "![a](Images/a11.png)\n",
    "\n",
    "The partial derivate for $w_{i}$  is just $x_{1}$.\n",
    "\n",
    "![a](Images/a12.png)\n",
    "\n",
    "Putting it all together, we get\n",
    "\n",
    "![a](Images/a15.png)\n",
    "\n",
    "We can simply define an \"ERROR TERM\" as  $δ = (y - \\hat{y}) * f'(h)$\n",
    "\n",
    "and write our weight update as:\n",
    "\n",
    "$w_{i}' = w_{i} + η * δ * x_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent: The Code \n",
    "\n",
    "From before we saw that one weight update can be calculated as:\n",
    "\n",
    "$\\Delta w_i = \\eta \\, \\delta x_i$ \n",
    "\n",
    "with the error term δ as\n",
    "\n",
    "$\\delta = (y - \\hat y) f'(h) = (y - \\hat y) f'(\\sum w_i x_i)$\n",
    "\n",
    "Remember, in the above equation $(y - \\hat y)$ is the output error, and f'(h) refers to the derivative of the activation function, f(h). We'll call that derivative the output gradient.\n",
    "\n",
    "Now I'll write this out in code for the case of only one output unit. We'll also be using the sigmoid as the activation function f(h).\n",
    "\n",
    "So f(h) = 1/(1+ exp(-x))\n",
    "\n",
    "f'(h) = 1/(1+ exp(-x)) * (1 - 1/(1+ exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# Activation Function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of Activation Function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 -sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.6899744811276125\n",
      "Amount of Error:\n",
      "-0.1899744811276125\n",
      "Change in Weights:\n",
      "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]\n"
     ]
    }
   ],
   "source": [
    "learnrate = 0.5\n",
    "x = np.array([1,2,3,4])\n",
    "y = np.array(0.5)\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "# Gradient Descent Step:\n",
    "\n",
    "\n",
    "# Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x,w)\n",
    "# not h = x * w\n",
    "\n",
    "# Calculate the ouptut \n",
    "y_hat  = sigmoid(h)\n",
    "\n",
    "# Calculate error of the ouput\n",
    "error = y - y_hat\n",
    "\n",
    "# Calculate the error term\n",
    "error_term = error * sigmoid_prime(h)\n",
    "\n",
    "# Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(y_hat)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing gradient descent\n",
    "\n",
    "Okay, now we know how to update our weights:\n",
    "\n",
    "$\\Delta w_{ij} = \\eta * \\delta_j * x_i$\n",
    "\n",
    "You've seen how to implement that for a single update, but how do we translate that code to calculate many weight updates so our network will learn?\n",
    "\n",
    "As an example, I'm going to have you use gradient descent to train a network on graduate school admissions data (found at http://www.ats.ucla.edu/stat/data/binary.csv). This dataset has three input features: GRE score, GPA, and the rank of the undergraduate school (numbered 1 through 4). Institutions with rank 1 have the highest prestige, those with rank 4 have the lowest.\n",
    "\n",
    "![a](Images/a16.png)\n",
    "\n",
    "The goal here is to predict if a student will be admitted to a graduate program based on these features. For this, we'll use a network with one output layer with one unit. We'll use a sigmoid function for the output unit activation.\n",
    "\n",
    "# Data Cleanup\n",
    "\n",
    "You might think there will be three input units, but we actually need to transform the data first. The rank feature is categorical, the numbers don't encode any sort of relative values. Rank 2 is not twice as much as rank 1, rank 3 is not 1.5 more than rank 2. Instead, we need to use dummy variables to encode rank, splitting the data into four new columns encoded with ones or zeros. Rows with rank 1 have one in the rank 1 dummy column, and zeros in all other columns. Rows with rank 2 have one in the rank 2 dummy column, and zeros in all other columns. And so on.\n",
    "\n",
    "We'll also need to standardize the GRE and GPA data, which means to scale the values such that they have zero mean and a standard deviation of 1. This is necessary because the sigmoid function squashes really small and really large inputs. The gradient of really small and large inputs is zero, which means that the gradient descent step will go to zero too. Since the GRE and GPA values are fairly large, we have to be really careful about how we initialize the weights or the gradient descent steps will die off and the network won't train. Instead, if we standardize the data, we can initialize the weights easily and everyone is happy.\n",
    "\n",
    "Now that the data is ready, we see that there are six input features: gre, gpa, and the four rank dummy variables. Here is an table of the data.\n",
    "\n",
    "![a](Images/a17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Square Error\n",
    "\n",
    "We're going to make a small change to how we calculate the error here. Instead of the SSE, we're going to use the mean of the square errors (MSE). Now that we're using a lot of data, summing up all the weight steps can lead to really large updates that make the gradient descent diverge.\n",
    "\n",
    "To compensate for this, you'd need to use a quite small learning rate. Instead, we can just divide by the number of records in our data, mm to take the average.This way, no matter how much data we use, our learning rates will typically be in the range of 0.01 to 0.001. Then, we can use the MSE (shown below) to calculate the gradient and the result is the same as before, just averaged instead of summed.\n",
    "\n",
    "$E = (1/2m) \\sum_{\\mu} (y^\\mu - \\hat{y}^\\mu)^2 $\n",
    "\n",
    "Here's the general algorithm for updating the weights with gradient descent:\n",
    "\n",
    "- Set the weight step to zero: $\\Delta w_i = 0$\n",
    "\n",
    "For each record in the training data\n",
    " 1. calculate the output $\\hat y = f(\\sum_i w_i x_i)$\n",
    " 2. calculate the error term, $δ=(y − \\hat{y}) * f'(\\sum_{i} w_{i}x_{i})$\n",
    " 3. update the weight, $Δw' =Δw +δx_{i}$\n",
    "\n",
    "- Update the weights, $w = w + ηΔw / m$ , where η is the learning rate and mm is the number of records.\n",
    "- Repeat for set epochs\n",
    "\n",
    "Our Activation Function remains sigmoid \n",
    "\n",
    "$f(h) = 1/(1+e^{-h})$ , and \n",
    "\n",
    "\n",
    "$f'(h) = f(h)(1 - f(h))$  so $f'(h) = 1/(1+e^{-h}) * (1 - 1/(1+e^{-h}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using numpy to implement\n",
    "\n",
    "1. Initialize the weights\n",
    "       - Must bbe near 0 and not squashed at high or low ends\n",
    "       - Must be random, so use normal distribution centered at 0\n",
    "   \n",
    " Use a scale: $1/ \\sqrt{n}$ , where n is the number of inputs. This keeps the input low for the number of inputs. So:\n",
    " \n",
    "```bash\n",
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "\n",
    "```\n",
    "\n",
    " 2. Use the np.dot functions to find dot product of two matrices, which is linear combinination of w and x.\n",
    " \n",
    "```bash\n",
    "# input to the output layer\n",
    "output_in = np.dot(weights, inputs)\n",
    "\n",
    "```\n",
    " \n",
    "3. Split training and testing 90/10\n",
    "\n",
    "```bash\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.loc[sample], data.drop(sample)\n",
    "```\n",
    "\n",
    "4. Splitting Features and Targets\n",
    "```bash\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "admissions = pd.read_csv('intro_to_neural_network_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   admit       gre       gpa  rank_1  rank_2  rank_3  rank_4\n",
      "0      0 -1.798011  0.578348       0       0       1       0\n",
      "1      1  0.625884  0.736008       0       0       1       0\n",
      "2      1  1.837832  1.603135       1       0       0       0\n",
      "3      1  0.452749 -0.525269       0       0       0       1\n",
      "4      0 -0.586063 -1.208461       0       0       0       1\n",
      "5      1  1.491561 -1.024525       0       1       0       0\n",
      "6      1 -0.239793 -1.077078       1       0       0       0\n",
      "7      0 -1.624876 -0.814312       0       1       0       0\n",
      "8      1 -0.412928  0.000263       0       0       1       0\n",
      "9      0  0.972155  1.392922       0       1       0       0\n"
     ]
    }
   ],
   "source": [
    "# Make dummy variables for rank, one hot encoding\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features to zero mean and a standard deviation of 1\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          gre       gpa  rank_1  rank_2  rank_3  rank_4\n",
      "209 -0.066657  0.289305       0       1       0       0\n",
      "280  0.625884  1.445476       0       1       0       0\n",
      "33   1.837832  1.603135       0       0       1       0\n",
      "210  1.318426 -0.131120       0       0       0       1\n",
      "93  -0.066657 -1.208461       0       1       0       0\n",
      "\n",
      " Targets: \n",
      "\n",
      "209    0\n",
      "280    0\n",
      "33     1\n",
      "210    0\n",
      "93     0\n",
      "Name: admit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split 90% to Training and 10% Testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.loc[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "\n",
    "print(features[:5])\n",
    "\n",
    "print (\"\\n Targets: \\n\")\n",
    "\n",
    "print(targets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def sigmoid(x): # x is dot product of weights and inputs\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.27086665326589826\n",
      "Train loss:  0.20672981036936744\n",
      "Train loss:  0.20074550171822744\n",
      "Train loss:  0.19871136118765292\n",
      "Train loss:  0.1978686940782158\n",
      "Train loss:  0.19746948362907232\n",
      "Train loss:  0.19726166263589326\n",
      "Train loss:  0.19714582811250664\n",
      "Train loss:  0.19707792744606575\n",
      "Train loss:  0.19703660044970267\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "\n",
    "#Initialize the weights\n",
    "n_records, n_features = features.shape\n",
    "weights = np.random.normal(scale = 1/n_features**0.5, size = n_features)\n",
    "\n",
    "#Training Parameters\n",
    "epochs = 1000\n",
    "leanrate = 0.5\n",
    "last_loss = None\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # Create a matrix matching shape of weights, this is delta w\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    \n",
    "    # Loop through all records, x is the input, y is the target\n",
    "    \n",
    "    # pandas.values removes any axis titles\n",
    "    for x,y in zip(features.values, targets):\n",
    "        \n",
    "        # Calculate the output\n",
    "        output = sigmoid(np.dot(x,weights))\n",
    "        \n",
    "        # Calculate the error\n",
    "        error = (y - output)\n",
    "        \n",
    "        # Sigmoid Prime\n",
    "        sigmoid_prime = (output)*(1-output)\n",
    "        \n",
    "        # Caluculate the error term\n",
    "        error_term = error * sigmoid_prime\n",
    "        \n",
    "        # Weight Change\n",
    "        del_w += error_term * x\n",
    "   \n",
    "    weights += (del_w * learnrate)/n_records\n",
    "    \n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        \n",
    "        # Mean Square Error\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        \n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "        \n",
    "    \n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons\n",
    "\n",
    "we have multiple input units and multiple hidden units, the weights between them will require two indices: $w_{ij}$ ,where $i$ denotes input units and $j$ are the hidden units.\n",
    "\n",
    "For example, the following image shows our network, with its input units labeled $x_1, x_2, x_3$ , and its hidden nodes labeled $h_1$ and $h_2$:\n",
    "\n",
    "![a](Images/a20.png)\n",
    "\n",
    "Now to index the weights, we take the input unit number for the i and the hidden unit number for the j. Now, the weights need to be stored in a matrix, indexed as $w_{ij}$. \n",
    "\n",
    "- Each row in the matrix will correspond to the weights leading out of a single input unit\n",
    "\n",
    "- Each column will correspond to the weights leading in to a single hidden unit.\n",
    "\n",
    "![a](Images/a21.png)\n",
    "\n",
    "To initialize these weights in NumPy, we have to provide the shape of the matrix. If features is a 2D array containing the input data:\n",
    "\n",
    "```bash\n",
    "\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "\n",
    "```\n",
    "\n",
    "So for each hidden layer unit, we need to calculate the following:\n",
    "\n",
    "$h_{j} = \\sum_{i} w_{ij}x_{i}$\n",
    "\n",
    "In this case, we're multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit, j = 1j=1, you'd take the dot product of the inputs with the first column of the weights matrix, like so:\n",
    "\n",
    "![a](Images/a22.png)\n",
    "\n",
    "we get this for the first hidden perceptron unit:\n",
    "\n",
    "$h_{1} = x_{1}w_{11} + x_{2}w_{21} + x_{3}w_{31}$\n",
    "\n",
    "\n",
    "In NumPy, you can do this for all the inputs and all the outputs at once using np.dot:\n",
    "\n",
    "```bash\n",
    "hidden_inputs = np.dot(inputs, weights_input_to_hidden)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Making a column vector\n",
    "\n",
    "Sometimes you'll want a column vector, even though by default NumPy arrays work like row vectors. For example when you need to dot (2x3) matrix with (1x3).  It's possible to get the transpose of an array like so arr.T, but for a 1D array, the transpose will return a row vector. Instead, use arr[:,None] to create a column vector:matrix.\n",
    "\n",
    "1. For 1-D vector use arr[:,None]\n",
    "2. For vector greater than 1-D, use transpose arr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.2 0.3]\n",
      "[0.5 0.2 0.3]\n",
      "[[0.5]\n",
      " [0.2]\n",
      " [0.3]]\n",
      "[[0.5 0.2 0.3]\n",
      " [0.1 2.3 1.3]]\n",
      "(2, 3)\n",
      "[[0.5 0.1]\n",
      " [0.2 2.3]\n",
      " [0.3 1.3]]\n"
     ]
    }
   ],
   "source": [
    "# 1-D\n",
    "one_D = np.array([0.5,0.2,0.3])\n",
    "print(one_D)\n",
    "\n",
    "# now we want column, doesnt work with .T\n",
    "print(one_D.T)\n",
    "\n",
    "#works with arr[:,None]\n",
    "print(one_D[:,None])\n",
    "\n",
    "two_D = np.array([[0.5,0.2,0.3],[0.1,2.3,1.3]])\n",
    "print(two_D)\n",
    "print(two_D.shape)\n",
    "print(two_D.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I'll implement a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the input features:\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986] \n",
      "\n",
      "Here are the hidden layer input weights: [4x3]\n",
      " [[-0.02341534 -0.0234137   0.15792128]\n",
      " [ 0.07674347 -0.04694744  0.054256  ]\n",
      " [-0.04634177 -0.04657298  0.02419623]\n",
      " [-0.19132802 -0.17249178 -0.05622875]] \n",
      "\n",
      "Hidden-layer Output: Sigmoid([1x4] Features dot [4x3] Weights = [1x3])\n",
      "[0.41492192 0.42604313 0.5002434 ]\n",
      "\n",
      "Here are the output layer input weights: [3x2]\n",
      " [[-0.10128311  0.03142473]\n",
      " [-0.09080241 -0.14123037]\n",
      " [ 0.14656488 -0.02257763]]\n",
      "\n",
      "Output-layer Output: Sigmoid([1x3] Features dot [3x2] Weights = [1x2])\n",
      "[0.49815196 0.48539772]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network Size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "print(\"Here are the input features:\")\n",
    "print(X,\"\\n\")\n",
    "\n",
    "# Define Random Weights np.random.normal(mean,std dev, dim)\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "print(\"Here are the hidden layer input weights: [4x3]\\n\" , weights_input_to_hidden, \"\\n\")\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "#Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output: Sigmoid([1x4] Features dot [4x3] Weights = [1x3])')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print(\"\\nHere are the output layer input weights: [3x2]\\n\" , weights_hidden_to_output)\n",
    "\n",
    "print('\\nOutput-layer Output: Sigmoid([1x3] Features dot [3x2] Weights = [1x2])')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Now we've come to the problem of how to make a multilayer neural network learn. Before, we saw how to update weights with gradient descent. The backpropagation algorithm is just an extension of that, using the chain rule to find the error with the respect to the weights connecting the input layer to the hidden layer (for a two layer network).\n",
    "\n",
    "Since we know the error at the output, we can use the weights to work backwards to hidden layers. For example, in the output layer, you have errors $\\delta^o_k$ attributed to each output unit k. Then, the error attributed to hidden unit $j$ is the output errors, scaled by the weights between the output and hidden layers (and the gradient):\n",
    "\n",
    "Error between hidden and output: $\\delta^h_j = \\sum W_{jk} \\delta^o_k f'(h_j)$\n",
    "\n",
    "Then, the gradient descent step is the same as before, just with the new errors: $\\Delta W_{ij} = \\eta \\delta^h_j x_i$\n",
    "\n",
    "where $w_{ij}$ are the weights between the inputs and hidden layer and $x_i$ are input unit values. This form holds for however many layers there are. \n",
    "\n",
    "The weight steps are equal to the step size times the output error of the layer times the values of the inputs to that layer: $\\Delta W_{pq} = \\eta  \\delta_{output}  V_{in}$\n",
    "\n",
    "Here, you get the output error, $\\delta_{output}$, by propagating the errors backwards from higher layers. And the input values, $V_{in}$ are the inputs to the layer, the hidden layer activations to the output unit for example.\n",
    "\n",
    "\n",
    "This is waaaay to hard to understand, so lets work through an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Backpropagation\n",
    "\n",
    "Let's walk through the steps of calculating the weight updates for a simple two layer network.\n",
    "\n",
    "Suppose there are two input values, one hidden unit , and one output unit with sigmoid activation on hidden and output.\n",
    "\n",
    "![a](Images/a23.png)\n",
    "\n",
    "1. First calculate the input to hidden layer:\n",
    "$h=∑w_i x_i = 0.1×0.4−0.2×0.3=−0.02$\n",
    "\n",
    "\n",
    "2. Calculate the output of hidden layer:\n",
    "$a=f(h)=sigmoid(−0.02)=0.495$\n",
    "\n",
    "\n",
    "3. Calculate the input to the output layer:\n",
    "$h=∑w_h x_h = 0.1×0.495= 0.0495$\n",
    "\n",
    "\n",
    "4. Calculate the output of output layer:\n",
    "$\\hat{y} = sigmoid(0.0495)=0.512$\n",
    "\n",
    "\n",
    "With the network output, we can start the backwards pass to calculate the weight updates for both layers. Using the fact that for the sigmoid function \n",
    "\n",
    "$f'(W \\cdot a) = f(W \\cdot a) (1 - f(W \\cdot a))$\n",
    "\n",
    "So the error for the output layer is:\n",
    "\n",
    "$δ_o =(y−\\hat{y})f′(W⋅a)=(1−0.512)×0.512×(1−0.512)=0.122$\n",
    "\n",
    "Now we need to calculate the error term for the hidden unit with backpropagation. Here we'll scale the error term from the output unit by the weight $W$ connecting it to the hidden unit.\n",
    "\n",
    "$\\delta^h_j = \\sum W_{jk} \\delta^o_k f'(h_j)$  , so:\n",
    "\n",
    "$\\delta^h = W \\delta^o_k f'(h_j) = 0.1×(0.122)×[0.495×(1−0.495)] = 0.003$\n",
    "\n",
    "\n",
    "Now that we have the errors, we can calculate the gradient descent steps.The hidden to output weight step is the learning rate, times the output unit error, times the hidden unit activation value.\n",
    "\n",
    "$ΔW = ηδ^oa = 0.5×0.122×0.495 = 0.0302$\n",
    "\n",
    "Then, for the input to hidden weights $w_i$, it's the learning rate times the hidden unit error, times the input values.\n",
    "\n",
    "$Δw_i = ηδ^hx_i = (0.5×0.003×0.1,0.5×0.003×0.3) = (0.00015,0.00045)$\n",
    "\n",
    "## Vanishing Gradient\n",
    "From this example, you can see one of the effects of using the sigmoid function for the activations. The maximum derivative of the sigmoid function is 0.25, so the errors in the output layer get reduced by at least 75%, and errors in the hidden layer are scaled down by at least 93.75%! You can see that if you have a lot of layers, using a sigmoid activation function will quickly reduce the weight steps to tiny values in layers near the input. This is known as the vanishing gradient problem. Later in the course you'll learn about other activation functions that perform better in this regard and are more commonly used in modern network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing in NumPy\n",
    "\n",
    "Previously we were only dealing with error terms from one unit. Now, in the weight update, we have to consider the error for each unit in the hidden layer, $\\delta_j$\n",
    "\n",
    "$\\Delta w_{ij} = \\eta\\delta_jx_{i}$\n",
    "\n",
    "Steps: \n",
    "1. Calculate networks output error\n",
    "2. Calculate networks output error term\n",
    "3. Backpropragate to calculate the hidden later error term\n",
    "4. Calculate the change in weights that are required from backpropragation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer input:\n",
      " [ 0.24 -0.46]\n",
      "hidden layer output:\n",
      " [0.55971365 0.38698582]\n",
      "output layer input:\n",
      " -0.06012438223148006\n",
      "output layer output:\n",
      " 0.48497343084992534\n",
      "error from output:\n",
      "0.11502656915007464\n",
      "output error term:\n",
      "0.028730669543515018\n",
      "hidden error term:\n",
      "[ 0.00070802 -0.00204471]\n",
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "# Weights of input hidden are [3x2]\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "#Foward Pass \n",
    "hidden_layer_input = np.dot(x,weights_input_hidden) #[1x3] dot [3x2] = [1x2]\n",
    "print(\"hidden layer input:\\n\", hidden_layer_input)\n",
    "\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "print(\"hidden layer output:\\n\", hidden_layer_output) \n",
    "\n",
    "#[1x2] dot [1x2] = [1]\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "print(\"output layer input:\\n\", output_layer_in)\n",
    "\n",
    "output = sigmoid(output_layer_in)\n",
    "print(\"output layer output:\\n\", output)\n",
    "\n",
    "## Backwards pass\n",
    "\n",
    "#Calculate output error\n",
    "error = target - output\n",
    "print('error from output:')\n",
    "print(error)\n",
    "\n",
    "#Calculate error term for output layer\n",
    "output_error_term = error * (output) * (1-output)\n",
    "print('output error term:')\n",
    "print(output_error_term)\n",
    "\n",
    "#Calculate error term for hidden layer\n",
    "hidden_error_term = weights_hidden_output * output_error_term * (hidden_layer_output) * (1-hidden_layer_output)\n",
    "print('hidden error term:')\n",
    "print(hidden_error_term)\n",
    "\n",
    "#Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "#Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:,None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
