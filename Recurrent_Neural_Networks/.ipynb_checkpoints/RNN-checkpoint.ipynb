{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "RNNs are based on the same principles as those behind FFNNs, which is why we spent so much time reminding ourselves of the feedforward and backpropagation steps that are used in the training phase.\n",
    "\n",
    "There are two main differences between FFNNs and RNNs. The Recurrent Neural Network uses:\n",
    "\n",
    "1. ***sequences*** as inputs in the training phase, and\n",
    "2. ***memory*** elements\n",
    "\n",
    "Memory is defined as the output of hidden layer neurons, which will serve as additional input to the network during next training step.\n",
    "\n",
    "<img src=\"rnn_img/m9.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Where:\n",
    "- y is the output\n",
    "- x is the input\n",
    "- s is the temporal dependancy\n",
    "\n",
    "### Applications:\n",
    "\n",
    "1. Sentiment Analysis\n",
    "2. Speech Recognition\n",
    "3. Time Series Prediction\n",
    "4. Natural Language Processing\n",
    "5. Gesture Recognition\n",
    "\n",
    "<img src=\"rnn_img/m10.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<img src=\"rnn_img/m11.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<img src=\"rnn_img/m12.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "As we've see, in FFNN the output at any time t, is a function of the current input and the weights. This can be easily expressed using the following equation:\n",
    "\n",
    "$y_t = F(x_t, W)$\n",
    "\n",
    "In RNNs, our output at time t, depends not only on the current input and the weight, but also on previous inputs. In this case the output at time t will be defined as:\n",
    "\n",
    "<img src=\"rnn_img/m13.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "This is the RNN folded model:\n",
    "\n",
    "<img src=\"rnn_img/m14.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "In this picture, $\\bar{x}$ represents the input vector, $\\bar{y}$ represents the output vector and $\\bar{s}$ denotes the state vector.\n",
    "\n",
    "$W_x$ is the weight matrix connecting the inputs to the state layer.\n",
    "\n",
    "$W_y$ is the weight matrix connecting the state layer to the output layer.\n",
    "\n",
    "$W_s$ represents the weight matrix connecting the state from the previous timestep to the state in the current timestep.\n",
    "\n",
    "\n",
    "\n",
    "### \"Unfolded in time\". The unfolded model is usually what we use when working with RNNs.\n",
    "\n",
    "<img src=\"rnn_img/m15.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "\n",
    "In FFNNs the hidden layer depended only on the current inputs and weights, as well as on an activation function $\\Phi$ in the following way:\n",
    "\n",
    "$\\bar{h'} = (\\bar{x} W^1 ) $\n",
    "\n",
    "In RNNs the state layer depended on the current inputs, their corresponding weights, the activation function and also on the previous state:\n",
    "\n",
    "<img src=\"rnn_img/w13.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n",
    "The output vector is calculated exactly the same as in FFNNs. It can be a linear combination of the inputs to each output node with the corresponding weight matrix $W_y$, or a softmax function of the same linear combination.\n",
    "\n",
    "$y_t = Ïƒ(\\bar{s}_t W_y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfolded Model\n",
    "\n",
    "The Elman Network (Unfolded Network) is usually modelled bottom up.\n",
    "\n",
    "<img src=\"rnn_img/m16.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "State inputs are the outputs from the previous foward pass, they are fed into the hidden layers with the inputs.\n",
    "\n",
    "<img src=\"rnn_img/m18.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Through Time\n",
    "\n",
    "We are now ready to understand how to train the RNN.\n",
    "\n",
    "When we train RNNs we also use backpropagation, but with a conceptual change. The process is similar to that in the FFNN, with the exception that we need to consider previous time steps, as the system has memory. This process is called Backpropagation Through Time (BPTT).\n",
    "\n",
    "We will use the Loss Function for our error. The Loss Function is the square of the difference between the desired and the calculated outputs. There are variations to the Loss Function, for example, factoring it with a scalar. In the backpropagation example we used a factoring scalar of 1/2 for calculation convenience.\n",
    "\n",
    "As described previously, the two most commonly used are the Mean Squared Error (MSE) (usually used in regression problems) and the cross entropy (usually used in classification problems). We are using a variation of the MSE.\n",
    "\n",
    "The state vector $\\bar{s}_t$ is calculated the following way:\n",
    "\n",
    "<img src=\"rnn_img/m20.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "The output vector $\\bar{y}_t$ can be product of the state vector $\\bar{s}_t$ and the corresponding weight elements of matrix $W_y$ As mentioned before, if the desired outputs are between 0 and 1, we can also use a softmax function. The following set of equations depicts these calculations:\n",
    "\n",
    "<img src=\"rnn_img/m21.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "As mentioned before, for the error calculations we will use the Loss Function, where:\n",
    "\n",
    "$E_t$ represents the output error at time t\n",
    "\n",
    "$d_t$ represents the desired output at time t\n",
    "\n",
    "$y_t$ represents the calculated output at time t\n",
    "\n",
    "<img src=\"rnn_img/m22.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "In BPTT we train the network at timestep t as well as take into account all of the previous timesteps.\n",
    "\n",
    "The easiest way to explain the idea is to simply jump into an example.\n",
    "\n",
    "In this example we will focus on the BPTT process for time step t=3. You will see that in order to adjust all three weight matrices, $W_x$, $W_s$ and $W_yW$, we need to consider timestep 3 as well as timestep 2 and timestep 1.\n",
    "\n",
    "As we are focusing on timestep t=3, the Loss function will be: $E_3=(\\bar{d}_3-\\bar{y}_3)^2$\n",
    "\n",
    "<img src=\"rnn_img/m23.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "To update each weight matrix, we need to find the partial derivatives of the Loss Function at time 3, as a function of all of the weight matrices. We will modify each matrix using gradient descent while considering the previous timesteps.\n",
    "\n",
    "<img src=\"rnn_img/m24.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "We will now unfold the model. You will see that unfolding the model in time is very helpful in visualizing the number of steps (translated into multiplication) needed in the Backpropagation Through Time process. These multiplications stem from the chain rule and are easily visualized using this model.\n",
    "\n",
    "In this video we will understand how to use Backpropagation Through Time (BPTT) when adjusting two weight matrices:\n",
    "\n",
    "$W_y$ - the weight matrix connecting the state the output\n",
    "\n",
    "$W_s$ - the weight matrix connecting one state to the next state\n",
    "\n",
    "<img src=\"rnn_img/m25.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "The partial derivative of the Loss Function with respect to $W_y$ is found by a simple one step chain rule: (Note that in this case we do not need to use BPTT.\n",
    "\n",
    "<img src=\"rnn_img/m27.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "Generally speaking, we can consider multiple timesteps back, and not only 3 as in this example. For an arbitrary timestep N, the gradient calculation needed for adjusting $W_y$ is:\n",
    "\n",
    "<img src=\"rnn_img/m28.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "<img src=\"rnn_img/m30.png\" alt=\"drawing\" width=\"1200\"/>\n",
    "\n",
    "<img src=\"rnn_img/m31.png\" alt=\"drawing\" width=\"1200\"/>\n",
    "\n",
    "We still need to adjust $W_x$ , the weight matrix connecting the input to the state.\n",
    "\n",
    "<img src=\"rnn_img/m32.png\" alt=\"drawing\" width=\"1200\"/>\n",
    "\n",
    "<img src=\"rnn_img/m33.png\" alt=\"drawing\" width=\"1200\"/>\n",
    "\n",
    "<img src=\"rnn_img/m34.png\" alt=\"drawing\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "<img src=\"rnn_img/m35.png\" alt=\"drawing\" width=\"1200\"/>\n",
    "\n",
    "Possible Activation Functions:\n",
    "\n",
    "1. Hyperbolic Tangent\n",
    "2. Sigmoid\n",
    "3. ReLu\n",
    "\n",
    "<img src=\"rnn_img/m41.png\" alt=\"drawing\" width=\"1200\"/>\n",
    "\n",
    "Gradient Clipping is simply normalizing the gradient values to a threshold. By doing this exploding gradients are managed reasonably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
