{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning\n",
    "\n",
    "Hyperparameter - A variable that we need to set before applying it within an algorithm\n",
    "\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "minibatch_size = 32\n",
    "epochs = 12\n",
    "\n",
    "```\n",
    "\n",
    "There are no set value for hyperparameters, each value depends on the task and dataset. We can generalize hyperparamters into two categories:\n",
    "\n",
    "Optimzizer Hyperparameters: \n",
    "    \n",
    "    1. Learning Rate\n",
    "    2. Minibatch size\n",
    "    3. # of training iteration\n",
    " \n",
    "Model Hyperparameters:\n",
    "\n",
    "    1. # of Hidden Layers\n",
    "    2. Model specific model parameters\n",
    "    \n",
    " \n",
    "## Learning Rate\n",
    "\n",
    "\" The single most important hyperparameter iand one should always make sure that is had been tuned\" - Yoshua Bengio\n",
    "\n",
    "A good starting point is 0.01 but the usual suspects are:\n",
    "\n",
    "```python\n",
    "0.1\n",
    "0.01\n",
    "0.001\n",
    "0.0001\n",
    "0.00001\n",
    "0.000001\n",
    "``` \n",
    "\n",
    "What is the intuition of the learning rate? \n",
    "\n",
    "Recall, we use gradient descednt to train our neural network model. The task boils down to decreasing the error value calculated by a loss function.\n",
    "\n",
    "<img src='rnn_img/m43.png' width=40% />\n",
    "\n",
    "Suppose we have a graph that model the Weights vs Error on a graph. The learning rate is the multiplier used to make a step closer to the local minimum. If the learning rate is too large the weights will never achieve the ideal error value. On the flip side, if the error rate is too low the model may never achieve a reasonable error.\n",
    "\n",
    "<img src='rnn_img/w44.png' width=80% />\n",
    "\n",
    "We don't exactly know the shapes of the curve, thus it may be more diffuct than we think. Remember the we have several weight that result in a plane in x-dimensions possibly.\n",
    "\n",
    "## Minibatch Size\n",
    "\n",
    "This hyperparameter impacts the resource requirement and training speed which may not be as trivial as other parametters. Researchers have been arguing about several training approaches in this regard.\n",
    "\n",
    "1. Online (Stochastic) -> Training on one example at at time.\n",
    "\n",
    "\n",
    "2. Batch - Feed the entire dataset and have the model train on iterations.\n",
    "\n",
    "The instructions today are to set a minibatch size, where the two extremes are:\n",
    "\n",
    "```python\n",
    "    minibatch_size = 1  #Stochastic\n",
    "    or\n",
    "    minibatch_size = \"# of training examples\" #Batch\n",
    "```\n",
    "\n",
    "The general minibatch sizes are: 1, 2, 4, 8, 16, 32, 64, 128, 256 (32 is most common)\n",
    "\n",
    "For Larger Minibatch:\n",
    "    \n",
    "    Pros: Allows to maximize the number of calculation done on the datset\n",
    "    Cons: More memory, computational intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
