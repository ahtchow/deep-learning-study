{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Network\n",
    "\n",
    "Useful Resources:\n",
    "\n",
    "1. [Chris Olah's LSTM post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "2. [Edwin Chen's LSTM post](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
    "3. [Andrej Karpathy's lecture on RNNs and LSTMs from CS231n](https://www.youtube.com/watch?v=iX5V1WpxxkY)\n",
    "\n",
    "## RNN vs LSTM\n",
    "\n",
    "Suppose we are trying to classify a wolf in a neural network. The outputs from the sigmoid function:\n",
    "\n",
    "- P(dog) = 0.8\n",
    "- P(wolf) = 0.15\n",
    "- P(fish) = 0.05\n",
    "\n",
    "What if this image is actually a wolf, not a dog? The previous images were a bear and a fox. How do we hint that that the image is a wolf, not a dog? We pass the previous data into the network classifier.\n",
    "\n",
    "<img src=\"rnn_img/c1.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "We can just combine the past data as a vector, then feed it through a linear function followed by an activation function (Sigmoid or hyperbolic tan). The main drawback of RNN is that information is lost over long timer intervals, this problem is known as ***Vanishing Gradient***. This is where the LSTM comes in. LSTMs allow long and short term memory to be passed into the cell and updates the long/short memory values based on the output.\n",
    "\n",
    "<img src=\"rnn_img/c2.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "## Basics of LSTM\n",
    "\n",
    "<img src=\"rnn_img/c3.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "The LSTM Cell is composed of 4 Gates:\n",
    "\n",
    "1. Forget Gate\n",
    "2. Learn Gate\n",
    "3. Remeber Gate\n",
    "4. Use Gate\n",
    "\n",
    "<img src=\"rnn_img/c4.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Long-Term Memory is passed to Forget Gate\n",
    "2. Short Term Memory and Event is Learn Gate\n",
    "3. Key Info from the Info Gate and Forget Gate are combined to form the Remeber Gate which will be the New Long Term Memory\n",
    "4. Key Info from the Learn Gate and Forget Gate are combined to form the Use Gate which is used to output the prediction and form the new short term memory.\n",
    "\n",
    "<img src=\"rnn_img/c5.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "\n",
    "## Architecture of LSTM\n",
    "\n",
    "The LSTM can be diagramed as the following:\n",
    "\n",
    "<img src=\"rnn_img/c7.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "### 1. Learn Gate\n",
    "\n",
    "1. Takes short term memory and event and combines the info\n",
    "2. Takes combined info and filters out unneccessary data\n",
    "\n",
    "<img src=\"rnn_img/c8.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "In the following example the Learn Gate takes the wolf (event) and the squirrel & tree (Short term) as inputs. The information is then combined and then proceeds to be filtered. The data from the tree is ignored due since it is meaningless in classification.\n",
    "\n",
    "The output of the Learn Gate is $N_ti_t$ where:\n",
    "\n",
    "<img src=\"rnn_img/c10.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "- $STM_{t-1}$ is the short term memory\n",
    "- $E_T$ is the event\n",
    "\n",
    "The new information $N_t$ is then matrix multiplied by an ignore factor $i_t$\n",
    "\n",
    "### 2. Forget Gate\n",
    "\n",
    "1. Long term memory is passed into forget gate\n",
    "2. Data from the long term memory is multiplied by a forget factor and is returned\n",
    "\n",
    "<img src=\"rnn_img/c11.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<img src=\"rnn_img/c12.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "- $LTM_{t-1}$ is the long term memory vector\n",
    "\n",
    "This is then multiplied by the forget factor with respect to the STM and the event. This is used to rid the memory of irrelevant data.\n",
    "\n",
    "### 3. Remember Gate\n",
    "\n",
    "1. Combines info from forget gate and learn gate to form new LTM\n",
    "\n",
    "<img src=\"rnn_img/c13.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "### 4. Use Gate\n",
    "\n",
    "To decide what info to use:\n",
    "\n",
    "1. Use the output from the forget gate and pass it through a tanh activation function.\n",
    "2. Consider the short term memory and event by passing it through a linear function, followed by a sigmoid activation function\n",
    "\n",
    "<img src=\"rnn_img/c14.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<img src=\"rnn_img/c16.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "3. Use $U_{t}V_{t}$ as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
