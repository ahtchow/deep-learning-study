{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Attention\n",
    "\n",
    "**\"One important property of human perception is that one does not tend to process a whole scene in its entirety at once. Instead humans focus attention selectively on parts of the visual space to aquire information when and where it is needed and combine information from different fixations voer time to build up and internal representation of the scene, guiding future eye movements and decision making.\"**\n",
    "\n",
    "In short, we do not process entire images when we look at a picture, rather we process parts sequentially.\n",
    "\n",
    "\n",
    "# Sequence to Sequence Models\n",
    "\n",
    "Attention was created to attend to the more important parts of text/ img data. Classic sequence to sequence models w/o attention must look at the data one time and produce every single part of the ouput. Attention allowed the state of the art neural translation models in which Google uses for its translation engine.\n",
    "\n",
    "Before we jump into learning about attention models, let's recap what you've learned about sequence to sequence models. We know that RNNs excel at using and generating sequential data, and sequence to sequence models can be used in a variety of applications!\n",
    "\n",
    "Applications: For any output that can represented as a sequence of vectors (Images, text)\n",
    "   \n",
    "   Sequence to Sequence: Takes in sequence of items, produces another sequence as output\n",
    "   \n",
    "    Train -> Output\n",
    "       \n",
    "    1. English Phrase -> French Phrase\n",
    "    2. News Article and Summary -> Summary (News Bot)\n",
    "    3. Questions and Answers-> Answers Model\n",
    "    4. Picture -> Object Labelling Model\n",
    "    \n",
    "    \n",
    "# Encoders and Decoders\n",
    "\n",
    "![encode](rnn_img/encoder-decoder.png)\n",
    "\n",
    "High-level Simplification:\n",
    "\n",
    " - Two Recurrent Nets (Encoder and Decoder)\n",
    " - Reads Input Sequence and Sends what it understands to the Decoder\n",
    " - This understanding is called a 'context state'\n",
    " - Decoder generates the output sequence.\n",
    "\n",
    "![encode](rnn_img/a1.png)\n",
    "\n",
    "   - Tokenize each word\n",
    "   - Feed one word at a time as a time step, while updating hidden state of an LSTM for e.g\n",
    "\n",
    "![encode](rnn_img/a2.png)\n",
    "\n",
    "   - fed the hidden state\n",
    "   - process each word and an associated response (may be response or text that follows)\n",
    "   \n",
    "![encode](rnn_img/a3.png)\n",
    "\n",
    "\n",
    "The encoder and decoder do not have to be RNNs; they can be CNNs too!\n",
    "\n",
    "In the example above, an LSTM is used to generate a sequence of words; LSTMs \"remember\" by keeping track of the input words that they see and their own hidden state.\n",
    "\n",
    "In computer vision, we can use this kind of encoder-decoder model to generate words or captions for an input image or even to generate an image from a sequence of input words. We'll focus on the first case: generating captions for images, and you'll learn more about caption generation in the next lesson. For now know that we can input an image into a CNN (encoder) and generate a descriptive caption for that image using an LSTM (decoder).\n",
    "\n",
    "## Recap for Sequence to Sequence\n",
    "\n",
    "![encode](rnn_img/a4.png)\n",
    "\n",
    "- Hidden states are updated for every item in a sequence\n",
    "- the final hidden state is sent to the decoder as the context state\n",
    "- the problem is that the encoder is confined to sending a single vector to represent a large sum of  words (e.g) , longer input sequence will have troubles\n",
    "- longer hidden states will cause overfitting for shorter text\n",
    "- ATTENTION solves this problem\n",
    "\n",
    "\n",
    "# Attention Encoder\n",
    "\n",
    "\n",
    "![encode](rnn_img/a5.png)\n",
    "\n",
    "- the encoder processes each input just like a sequence to sequence model without attenition\n",
    "- each word produces a hidden state\n",
    "- this time the all hidden states are passed onto the attention decoder\n",
    "- this capture greater context in order to capture more information\n",
    "- each hidden state is most associated with each input word of the sequence, but also capture a bit of the context around\n",
    "\n",
    "![encode](rnn_img/a7.png)\n",
    "\n",
    "A more detailed look:\n",
    "\n",
    "- Embeddings are created based on the input and are fed through the RNN\n",
    "- hidden states are generated based on the embeddings\n",
    "\n",
    "\n",
    "# Attention Decoder\n",
    "\n",
    "\n",
    "![encode](rnn_img/a8.png)\n",
    "\n",
    "- hidden states are scored individually\n",
    "- then feed through a softmax activation function\n",
    "- then the context state is created by summating the hidden state multiplied by softmax scores\n",
    "\n",
    "![encode](rnn_img/a9.png)\n",
    "\n",
    "- Generates the context vector\n",
    "- Passes through RNN which generates a new hidden state at each time step\n",
    "- Next time step takes prev hidden state, output, context state to output next vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
