{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Introduction\n",
    "\n",
    "The neural network architectures you've seen so far were trained using the current inputs only. We did not consider previous inputs when generating the current output. In other words, our systems did not have any memory elements. RNNs address this very basic and important issue by using memory (i.e. past inputs to the network) when producing the current output.\n",
    "\n",
    "\n",
    "## History\n",
    "- Feedfoward networks are limitied due to unablility to capture temperial depencies which limited applications.\n",
    "- First attempt was called the Time Delayed Neural Network (TDNN) in 1989, where past information had to be fed into the network. Disadvantage was that it could only have access on memory fed in.\n",
    "- Simple RNN or Elman Network was next in 1990, this introduced the vanishing gradient problem which prevented networks to trace back into data.\n",
    "- In mid 90's, LSTM were introduced to capture and store data w/ consideration of the vanishing gradient\n",
    "\n",
    "RNNs have a key flaw, as capturing relationships that span more than 8 or 10 steps back is practically impossible. This flaw stems from the \"vanishing gradient\" problem in which the contribution of information decays geometrically over time. As you may recall, while training our network we use backpropagation. In the backpropagation process we adjust our weight matrices with the use of a gradient. In the process, gradients are calculated by continuous multiplications of derivatives. The value of these derivatives may be so small, that these continuous multiplications may cause the gradient to practically \"vanish\".\n",
    "\n",
    "LSTM is one option to overcome the Vanishing Gradient problem in RNNs, by helping us apply networks that have temporal dependencies.\n",
    "\n",
    "## Applications\n",
    "- Are you into gaming and bots? Check out the DotA 2 bot by Open AI\n",
    "- How about automatically adding sounds to silent movies?\n",
    "- Here is a cool tool for automatic handwriting generation\n",
    "- Amazon's voice to text using high quality speech recognition, Amazon Lex.\n",
    "- Facebook uses RNN and LSTM technologies for building language models\n",
    "- Netflix also uses RNN models - here is an interesting read\n",
    "\n",
    "# Feedfoward Neural Network (Reminder)\n",
    "\n",
    "![a](rnn_img/m1.png)\n",
    "\n",
    "Stages:\n",
    "\n",
    "1. Training - During the training phase, we take the data set (also called the training set), which includes many pairs of inputs and their corresponding targets (outputs).\n",
    "\n",
    "    1a. Feedfoward\n",
    "    \n",
    "    1b. Backpropagation\n",
    "    \n",
    "    \n",
    "2. Evaluation - Use the network that was created in the training phase, apply our new inputs and expect to obtain the desired outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedfoward Process\n",
    "\n",
    "![a](rnn_img/m2.png)\n",
    "\n",
    "As you saw in the video above, vector h' of the hidden layer will be calculated by multiplying the input vector with the weight matrix $W^{1}$\n",
    "  the following way:\n",
    "\n",
    "$\\bar{h'} = (\\bar{x} W^1 ) $\n",
    "\n",
    "Using vector by matrix multiplication, we can look at this computation the following way:\n",
    "\n",
    "![a](rnn_img/m3.png)\n",
    "\n",
    "After finding $h'$\n",
    "  , we need an activation function $(\\Phi)$ to finalize the computation of the hidden layer's values. This activation function can be a Hyperbolic Tangent, a Sigmoid or a ReLU function. We can use the following two equations to express the final hidden vector $\\bar{h}$ \n",
    "\n",
    "$\\bar{h'} = \\Phi(\\bar{x} W^1 ) $\n",
    "\n",
    "Since $W_{ij}$ represents the weight component in the weight matrix, connecting neuron i from the input to neuron j in the hidden layer, we can also write these calculations in the following way: (notice that in this example we have n inputs and only 3 hidden neurons)\n",
    "\n",
    "![a](rnn_img/m4.png)\n",
    "\n",
    "The process of calculating the output vector is mathematically similar to that of calculating the vector of the hidden layer. We use, again, a vector by matrix multiplication, which can be followed by an activation function. The vector is the newly calculated hidden layer and the matrix is the one connecting the hidden layer to the output.\n",
    "\n",
    "![a](rnn_img/m5.png)\n",
    "\n",
    "Essentially, each new layer in an neural network is calculated by a vector by matrix multiplication, where the vector represents the inputs to the new layer and the matrix is the one connecting these new inputs to the next layer.\n",
    "\n",
    "In our example, the input vector is $\\bar{h}$ and the matrix is $W^2$\n",
    " , therefore $\\bar{y}=\\bar{h}W^2$ \n",
    " . In some applications it can be beneficial to use a softmax function (if we want all output values to be between zero and 1, and their sum to be 1).\n",
    " \n",
    " ![a](rnn_img/m6.png)\n",
    " \n",
    " \n",
    " The two error functions that are most commonly used are the Mean Squared Error (MSE) (usually used in regression problems) and the cross entropy (usually used in classification problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Process\n",
    "\n",
    "In the backpropagation process we minimize the network error slightly with each iteration, by adjusting the weights. If we look at an arbitrary layer k, we can define the amount by which we change the weights from neuron i to neuron j stemming from layer k as: $Î”W^k_{ij} $\n",
    "\n",
    "The superscript (k) indicates that the weight connects layer k to layer k+1.\n",
    "\n",
    "Therefore, the weight update rule for that neuron can be expressed as:\n",
    "\n",
    "$W_{new} = W_{previous} +\\Delta W^k_{ij}$\n",
    "\n",
    "<img src=\"rnn_img/m7.png\" alt=\"drawing\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "How handle overffiting\n",
    "\n",
    "![a](rnn_img/m8.png)\n",
    "\n",
    "1. Stop Early\n",
    "2. Regularization (Dropout) - Helps Generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
