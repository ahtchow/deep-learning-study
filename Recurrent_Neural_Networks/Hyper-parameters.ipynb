{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning\n",
    "\n",
    "Hyperparameter - A variable that we need to set before applying it within an algorithm\n",
    "\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "minibatch_size = 32\n",
    "epochs = 12\n",
    "\n",
    "```\n",
    "\n",
    "There are no set value for hyperparameters, each value depends on the task and dataset. We can generalize hyperparamters into two categories:\n",
    "\n",
    "Optimzizer Hyperparameters: \n",
    "    \n",
    "    1. Learning Rate\n",
    "    2. Minibatch size\n",
    "    3. # of training iteration\n",
    " \n",
    "Model Hyperparameters:\n",
    "\n",
    "    1. # of Hidden Layers\n",
    "    2. Model specific model parameters\n",
    "    \n",
    " \n",
    "## Learning Rate\n",
    "\n",
    "\" The single most important hyperparameter iand one should always make sure that is had been tuned\" - Yoshua Bengio\n",
    "\n",
    "A good starting point is 0.01 but the usual suspects are:\n",
    "\n",
    "```python\n",
    "0.1\n",
    "0.01\n",
    "0.001\n",
    "0.0001\n",
    "0.00001\n",
    "0.000001\n",
    "``` \n",
    "\n",
    "What is the intuition of the learning rate? \n",
    "\n",
    "Recall, we use gradient descednt to train our neural network model. The task boils down to decreasing the error value calculated by a loss function.\n",
    "\n",
    "<img src='rnn_img/m43.png' width=40% />\n",
    "\n",
    "Suppose we have a graph that model the Weights vs Error on a graph. The learning rate is the multiplier used to make a step closer to the local minimum. If the learning rate is too large the weights will never achieve the ideal error value. On the flip side, if the error rate is too low the model may never achieve a reasonable error.\n",
    "\n",
    "<img src='rnn_img/w44.png' width=80% />\n",
    "\n",
    "We don't exactly know the shapes of the curve, thus it may be more diffuct than we think. Remember the we have several weight that result in a plane in x-dimensions possibly.\n",
    "\n",
    "## Minibatch Size\n",
    "\n",
    "This hyperparameter impacts the resource requirement and training speed which may not be as trivial as other parametters. Researchers have been arguing about several training approaches in this regard.\n",
    "\n",
    "1. Online (Stochastic) -> Training on one example at at time.\n",
    "\n",
    "\n",
    "2. Batch - Feed the entire dataset and have the model train on iterations.\n",
    "\n",
    "The instructions today are to set a minibatch size, where the two extremes are:\n",
    "\n",
    "```python\n",
    "    minibatch_size = 1  #Stochastic\n",
    "    or\n",
    "    minibatch_size = \"# of training examples\" #Batch\n",
    "```\n",
    "\n",
    "The general minibatch sizes are: 1, 2, 4, 8, 16, 32, 64, 128, 256 (32 is most common)\n",
    "\n",
    "For Larger Minibatch:\n",
    "    \n",
    "    Pros: Allows to maximize the number of calculation done on the datset\n",
    "    Cons: More memory, computational intensity\n",
    "    \n",
    "In practice, smaller minibatch sizes have more noise which is often helpful in prevent the learning curve to get stuck on a local minima.\n",
    "\n",
    "<img src='rnn_img/m44.png' width=80% />\n",
    "\n",
    "In general, it is ideal to start at 32 and work your way up. A research paper shows the impact of different batch sizes on a CNN.\n",
    "\n",
    "<img src='rnn_img/w45.png' width=80% />\n",
    "\n",
    "Overall, too large could cause error and too small could be too slow.\n",
    "\n",
    "# Number of Training Iterations\n",
    "\n",
    "To choose the right number of iterations, we look at the Validation Error.\n",
    "As long as the validation error is decreasing, we can continue to increase the iterations.\n",
    "\n",
    "```python\n",
    "Epoch 1, Batch 1, Training Error: 4.4181, Validation Error: 4.5543\n",
    "Epoch 1, Batch 2, Training Error: 4.3181, Validation Error: 4.3543\n",
    "Epoch 1, Batch 3, Training Error: 4.5181, Validation Error: 4.1363\n",
    "Epoch 1, Batch 4, Training Error: 3.8181, Validation Error: 3.8233\n",
    "Epoch 1, Batch 5, Training Error: 3.7181, Validation Error: 3.4123\n",
    "```\n",
    "\n",
    "We can use a technique called early stopping which will stop the training early if the model has not improved for a x amount of training iterations.\n",
    "\n",
    "# Number of Hidden Units / Layers\n",
    "\n",
    "In order for a model to learn, it needs enough  \"capacity\". THe more complex the more learning capacity it has to learn. However, this also implies that the model can overfit and memorize the data easily rather than learn.\n",
    "\n",
    "<img src='rnn_img/w46.png' width=80% />\n",
    "\n",
    "If the training accuracy is greater than the validation accuracy, the you can decrease the number of hidden units. You may also utilize regularization techniques like dropout or L2 Regularization.\n",
    "\n",
    "\"in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers).\" ~ Andrej Karpathy in https://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "# LSTM Vs GRU (RNN)\n",
    "\n",
    "- \"Our results are not conclusive in comparing the LSTM and the GRU, which suggests that the choice of the type of gated recurrent unit may depend heavily on the dataset and corresponding task.\"\n",
    "\n",
    "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling by Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio\n",
    "\n",
    "- \"The GRU outperformed the LSTM on all tasks with the exception of language modelling\"\n",
    "\n",
    "An Empirical Exploration of Recurrent Network Architectures by Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskever\n",
    "\n",
    "- \"Our consistent finding is that depth of at least two is beneficial. However, between two and three layers our results are mixed. Additionally, the results are mixed between the LSTM and the GRU, but both significantly outperform the RNN.\"\n",
    "\n",
    "Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, Li Fei-Fei\n",
    "\n",
    "- \"In our [Neural Machine Translation] experiments, LSTM cells consistently outperformed GRU cells. Since the computational bottleneck in our architecture is the softmax operation we did not observe large difference in training speed between LSTM and GRU cells\"\n",
    "\n",
    "Massive Exploration of Neural Machine Translation Architectures by Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "If you want to learn more about hyperparameters, these are some great resources on the topic:\n",
    "\n",
    "- Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio\n",
    "\n",
    "- Deep Learning book - chapter 11.4: Selecting Hyperparameters by Ian Goodfellow, Yoshua Bengio, Aaron Courville\n",
    "\n",
    "- Neural Networks and Deep Learning book - Chapter 3: How to choose a neural network's hyper-parameters? by Michael Nielsen\n",
    "\n",
    "- Efficient BackProp (pdf) by Yann LeCun\n",
    "\n",
    "More specialized sources:\n",
    "\n",
    "- How to Generate a Good Word Embedding? by Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao\n",
    "- Systematic evaluation of CNN advances on the ImageNet by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas\n",
    "- Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, Li Fei-Fei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
